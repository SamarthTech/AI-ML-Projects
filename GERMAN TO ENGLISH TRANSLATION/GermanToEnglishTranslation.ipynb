{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b1232",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from numpy import array, argmax, random, take\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "pd.set_option('display.max_colwidth',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea191a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO READ RAW TEXT FILE\n",
    "\n",
    "def read_text(filename):\n",
    "    # open the file\n",
    "    file = open(filename, mode = 'rt', encoding = 'utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6077986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT A TEXT INTO SENTENCES\n",
    "\n",
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t') for i in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_text(\"/content/deu.txt\")\n",
    "deu_eng = to_lines(data)\n",
    "deu_eng = array(deu_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e711104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deu_eng = deu_eng[:50000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKING A LOOK AT OUR DATA:\n",
    "\n",
    "deu_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMPTY LISTS\n",
    "\n",
    "eng_l = []\n",
    "deu_l = []\n",
    "\n",
    "# POPULATE THE LISTS WITH SENTENCE LENGTHS\n",
    "\n",
    "for i in deu_eng[:,0]:\n",
    "    eng_l.append(len(i.split()))\n",
    "    \n",
    "for i in deu_eng[:,1]:\n",
    "    deu_l.append(len(i.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e8a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = pd.DataFrame({'eng':eng_l, 'deu':deu_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb02093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO BUILD A TOKENIZER\n",
    "\n",
    "def tokenization(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14886ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE ENGLISH TOKENIZER\n",
    "\n",
    "eng_tokenizer = tokenizer(deu_eng[:,0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "eng_length = 8\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DEUTCH TOKENIZER\n",
    "\n",
    "deu_tokenizer = tokenizer(deu_eng[:,1])\n",
    "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
    "\n",
    "deu_length = 8\n",
    "print('Deutch Vocabulary Size: %d' % deu_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48898e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE AND PAD SEQUENCES\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    seq = pad_sequences(seq, maxlen = length, padding = 'post')\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cee64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(deu_eng, test_size = 0.2, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE TRAINING DATA\n",
    "\n",
    "trainX = encode_sequences(deu_tokenizer, deu_length, train[:,1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de71805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE VALIDATION DATA\n",
    "\n",
    "testX = encode_sequences(deu_tokenizer, deu_length, train[:,1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD NMT MODEL\n",
    "\n",
    "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(in_vocab, units, input_length = in_timesteps, mask_zero = True))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(out_timestep))\n",
    "    model.add(LSTM(units, return_sequences = True))\n",
    "    model.add(Dense(out_vocab, activation = 'softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada419a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
    "rms = optimizer.RMSprop(lr = 0.001)\n",
    "model.compile(optimizer = rms, loss = 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b526132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.h1.deepra'\n",
    "checkpoint = ModelCheckpoint(filename, monitor = 'val_loss', verbose = 1, save_best_only = True, mode = 'min')\n",
    "\n",
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1],1),\n",
    "          epochs = 5, batch_size = 512,\n",
    "          validation_split = 0.2,\n",
    "          callbacks = [checkpoint],verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d500be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea89073",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h1.deepra')\n",
    "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceade9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n;\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT PREDICTIONS INTO TEXT (ENGLISH)\n",
    "\n",
    "preds_text = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], eng_tokenizer)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.apend(t)\n",
    "                \n",
    "        else:\n",
    "            if (t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                if(t == None):\n",
    "                    temp.append('')\n",
    "                else:\n",
    "                    temp.append(t)\n",
    "                    \n",
    "        preds_text.append(''.join(temp))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817aa09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e591de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d57306",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.tail(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
